{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import sacrebleu\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.datasets import Multi30k\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformer import Transformer\n",
    "from utils import *\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC, TRG = \"de\", \"en\"\n",
    "\n",
    "train_iter = Multi30k(split='train', language_pair=(SRC, TRG))\n",
    "f_de = open(\"Multi30k_de_text.txt\", \"w\")\n",
    "f_en = open(\"Multi30k_en_text.txt\", \"w\")\n",
    "for pair in train_iter:\n",
    "    f_de.write(pair[0]+'\\n')\n",
    "    f_en.write(pair[1]+'\\n')\n",
    "f_de.close()\n",
    "f_en.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab_size = 8200\n",
    "de_vocab_size = 10000\n",
    "vocab_sizes = {\"en\": en_vocab_size, \"de\": de_vocab_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=Multi30k_de_text.txt --model_prefix=Multi30k_de --user_defined_symbols= --vocab_size=10000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: Multi30k_de_text.txt\n",
      "  input_format: \n",
      "  model_prefix: Multi30k_de\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: Multi30k_de_text.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 29000 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=2075871\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.9552% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=59\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.999552\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 29000 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 59750 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 29000\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 24824\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 24824 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=16128 obj=10.2419 num_tokens=52489 num_tokens/piece=3.25453\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=13880 obj=7.87034 num_tokens=52837 num_tokens/piece=3.8067\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=10982 obj=7.85011 num_tokens=55839 num_tokens/piece=5.08459\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=10969 obj=7.81973 num_tokens=55870 num_tokens/piece=5.09345\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: Multi30k_de.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: Multi30k_de.vocab\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=Multi30k_en_text.txt --model_prefix=Multi30k_en --user_defined_symbols= --vocab_size=8200\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: Multi30k_en_text.txt\n",
      "  input_format: \n",
      "  model_prefix: Multi30k_en\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8200\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: Multi30k_en_text.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 29000 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=1801236\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.9516% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=52\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.999516\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 29000 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 25692 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 29000\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 15387\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 15387 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=10170 obj=9.03356 num_tokens=30133 num_tokens/piece=2.96293\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=8287 obj=7.02627 num_tokens=30222 num_tokens/piece=3.64692\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: Multi30k_en.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: Multi30k_en.vocab\n"
     ]
    }
   ],
   "source": [
    "# train sentencepiece models to get tokenizers\n",
    "spm.SentencePieceTrainer.train\\\n",
    "(f'--input=Multi30k_de_text.txt --model_prefix=Multi30k_de --user_defined_symbols= --vocab_size={de_vocab_size}')\n",
    "spm.SentencePieceTrainer.train\\\n",
    "(f'--input=Multi30k_en_text.txt --model_prefix=Multi30k_en --user_defined_symbols= --vocab_size={en_vocab_size}')\n",
    "\n",
    "# make SentencePieceProcessor instances and load the model files\n",
    "de_sp = spm.SentencePieceProcessor()\n",
    "de_sp.load('Multi30k_de.model')\n",
    "en_sp = spm.SentencePieceProcessor()\n",
    "en_sp.load('Multi30k_en.model')\n",
    "\n",
    "tokenizers = {\"en\": en_sp.encode_as_ids, \"de\": de_sp.encode_as_ids}\n",
    "detokenizers = {\"en\":en_sp.decode_ids, \"de\":de_sp.decode_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29000 1014\n",
      "('Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.', 'Two young, White males are outside near many bushes.')\n",
      "('Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.', 'Several men in hard hats are operating a giant pulley system.')\n",
      "('Ein kleines Mädchen klettert in ein Spielhaus aus Holz.', 'A little girl climbing into a wooden playhouse.')\n",
      "('Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.', 'A man in a blue shirt is standing on a ladder cleaning a window.')\n",
      "('Zwei Männer stehen am Herd und bereiten Essen zu.', 'Two men are at the stove preparing food.')\n",
      "('Ein Mann in grün hält eine Gitarre, während der andere Mann sein Hemd ansieht.', 'A man in green holds a guitar while the other man observes his shirt.')\n",
      "('Ein Mann lächelt einen ausgestopften Löwen an.', 'A man is smiling at a stuffed lion')\n",
      "('Ein schickes Mädchen spricht mit dem Handy während sie langsam die Straße entlangschwebt.', 'A trendy girl talking on her cellphone while gliding slowly down the street.')\n",
      "('Eine Frau mit einer großen Geldbörse geht an einem Tor vorbei.', 'A woman with a large purse is walking by a gate.')\n",
      "('Jungen tanzen mitten in der Nacht auf Pfosten.', 'Boys dancing on poles in the middle of the night.')\n"
     ]
    }
   ],
   "source": [
    "# indexes of special symbols\n",
    "UNK, BOS, EOS, PAD = 0, 1, 2, 3\n",
    "\n",
    "train_iter = Multi30k(split='train', language_pair=(SRC, TRG))\n",
    "valid_iter = Multi30k(split='valid', language_pair=(SRC, TRG))\n",
    "test_iter  = Multi30k(split='test',  language_pair=(SRC, TRG))\n",
    "\n",
    "train_set = [(x.rstrip('\\n'), y.rstrip('\\n')) for x, y in train_iter if x!='']\n",
    "valid_set = [(x.rstrip('\\n'), y.rstrip('\\n')) for x, y in valid_iter if x!='']\n",
    "# test_set  = [(x.rstrip('\\n'), y.rstrip('\\n')) for x, y in test_iter if x!='']\n",
    "print(len(train_set), len(valid_set))\n",
    "for i in range(10):\n",
    "   print(train_set[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 50\n",
    "def tokenize_dataset(dataset):\n",
    "    'tokenize a dataset and add [BOS] and [EOS] to the beginning and end of the sentences'\n",
    "    return [(torch.tensor([BOS]+tokenizers[SRC](src_text)[0:max_seq_len-2]+[EOS]),\n",
    "             torch.tensor([BOS]+tokenizers[TRG](trg_text)[0:max_seq_len-2]+[EOS]))\n",
    "            for src_text, trg_text in dataset]\n",
    "          \n",
    "train_tokenized = tokenize_dataset(train_set)\n",
    "valid_tokenized = tokenize_dataset(valid_set)\n",
    "# test_tokenized  = tokenize_dataset(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    'create a dataset for torch.utils.data.DataLoader() '\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "def pad_sequence(batch):\n",
    "    'collate function for padding sentences such that all \\\n",
    "    the sentences in the batch have the same length'\n",
    "    src_seqs  = [src for src, trg in batch]\n",
    "    trg_seqs  = [trg for src, trg in batch]\n",
    "    src_padded = torch.nn.utils.rnn.pad_sequence(src_seqs,\n",
    "                                batch_first=True, padding_value = PAD)\n",
    "    trg_padded = torch.nn.utils.rnn.pad_sequence(trg_seqs,\n",
    "                                batch_first=True, padding_value = PAD)\n",
    "    return src_padded, trg_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "class Dataloaders:\n",
    "    'Dataloaders contains train_loader, test_loader and valid_loader for training and evaluation '\n",
    "    def __init__(self):\n",
    "        self.train_dataset = TranslationDataset(train_tokenized)\n",
    "        self.valid_dataset = TranslationDataset(valid_tokenized)\n",
    "        # self.test_dataset  = TranslationDataset(test_tokenized)\n",
    "        \n",
    "        # each batch returned by dataloader will be padded such that all the texts in\n",
    "        # that batch have the same length as the longest text in that batch\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.train_dataset, batch_size=batch_size,\n",
    "                                                shuffle=True, collate_fn = pad_sequence)\n",
    "        \n",
    "        \n",
    "        self.valid_loader = torch.utils.data.DataLoader(self.valid_dataset, batch_size=batch_size,\n",
    "                                                shuffle=True, collate_fn=pad_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch_input(x, y):\n",
    "        src = x.to(device)\n",
    "        trg_in = y[:, :-1].to(device)\n",
    "        trg_out = y[:, 1:].contiguous().view(-1).to(device)\n",
    "        src_pad_mask = (src == PAD).view(src.size(0), 1, 1, src.size(-1))\n",
    "        trg_pad_mask = (trg_in == PAD).view(trg_in.size(0), 1, 1, trg_in.size(-1))\n",
    "        return src, trg_in, trg_out, src_pad_mask, trg_pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    model = Transformer(num_encoder_layers=6,\n",
    "                        num_decoder_layers=6,\n",
    "                        d_model=512,\n",
    "                        num_heads=8,\n",
    "                        dff=2048,\n",
    "                        input_vocab_size=vocab_sizes[SRC],\n",
    "                        target_vocab_size=vocab_sizes[TRG],\n",
    "                        max_seq_len=max_seq_len,\n",
    "                        dropout_rate=0.1).to(device)\n",
    "\n",
    "    # initialize model parameters\n",
    "    # it seems that this initialization is very important!\n",
    "    for p in model.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders = Dataloaders()\n",
    "\n",
    "model = make_model()\n",
    "\n",
    "warmup_steps = 3 * len(data_loaders.train_loader)\n",
    "# lr first increases in the warmup steps, and then descreases\n",
    "lr_fn = lambda step: 512 **(-0.5) * min([(step+1)**(-0.5), (step+1) * warmup_steps ** (-1.5)])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_fn)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "early_stop_count = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloaders):\n",
    "    model.train()\n",
    "    grad_norm_clip = 1.0\n",
    "    losses, acc, count = [], 0, 0\n",
    "    num_batches = len(dataloaders.train_loader)\n",
    "    pbar = tqdm(enumerate(dataloaders.train_loader), total=num_batches)\n",
    "    for idx, (x, y)  in  pbar:\n",
    "        optimizer.zero_grad()\n",
    "        src, trg_in, trg_out, src_pad_mask, trg_pad_mask = make_batch_input(x, y)\n",
    "        pred, _ = model(src, trg_in)\n",
    "        pred = pred.view(-1, pred.size(-1))\n",
    "        loss = loss_fn(pred, trg_out).to(device)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm_clip)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        losses.append(loss.item())\n",
    "        # report progress\n",
    "        if idx>0 and idx%50 == 0:\n",
    "            pbar.set_description(f'train loss={loss.item():.3f}, lr={scheduler.get_last_lr()[0]:.5f}')\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def train(model, dataloaders, epochs):\n",
    "    global early_stop_count\n",
    "    best_valid_loss = float('inf')\n",
    "    train_size = len(dataloaders.train_loader)*batch_size\n",
    "    for ep in range(epochs):\n",
    "        train_loss = train_epoch(model, dataloaders)\n",
    "        valid_loss = validate(model, dataloaders.valid_loader)\n",
    "        \n",
    "        print(f'ep: {ep}: train_loss={train_loss:.5f}, valid_loss={valid_loss:.5f}')\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "        else:\n",
    "            if scheduler.last_epoch>2*warmup_steps:\n",
    "                early_stop_count -= 1\n",
    "                if early_stop_count<=0:   \n",
    "                    return train_loss, valid_loss\n",
    "    return train_loss, valid_loss\n",
    "      \n",
    "               \n",
    "def validate(model, dataloder):\n",
    "    'compute the validation loss'\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(dataloder):\n",
    "            src, trg_in, trg_out, src_pad_mask, trg_pad_mask = make_batch_input(x,y)\n",
    "            pred, _ = model(src, trg_in)\n",
    "            pred = pred.view(-1, pred.size(-1))\n",
    "            losses.append(loss_fn(pred, trg_out).item())\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss=4.345, lr=0.00025: 100%|██████████| 227/227 [00:28<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0: train_loss=5.77885, valid_loss=4.05680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss=3.101, lr=0.00053: 100%|██████████| 227/227 [00:28<00:00,  7.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 1: train_loss=3.42724, valid_loss=2.97579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss=2.543, lr=0.00082: 100%|██████████| 227/227 [00:28<00:00,  7.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 2: train_loss=2.64849, valid_loss=2.45689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss=2.119, lr=0.00074: 100%|██████████| 227/227 [00:28<00:00,  7.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 3: train_loss=2.13015, valid_loss=2.09979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss=1.583, lr=0.00066: 100%|██████████| 227/227 [00:28<00:00,  7.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 4: train_loss=1.68488, valid_loss=1.81377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss=1.259, lr=0.00060: 100%|██████████| 227/227 [00:28<00:00,  7.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 5: train_loss=1.36163, valid_loss=1.63384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss=1.092, lr=0.00056: 100%|██████████| 227/227 [00:28<00:00,  7.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 6: train_loss=1.13870, valid_loss=1.56924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss=0.901, lr=0.00052: 100%|██████████| 227/227 [00:28<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 7: train_loss=0.97735, valid_loss=1.53548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss=0.855, lr=0.00049: 100%|██████████| 227/227 [00:28<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 8: train_loss=0.84089, valid_loss=1.47775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss=0.704, lr=0.00047: 100%|██████████| 227/227 [00:28<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 9: train_loss=0.71335, valid_loss=1.48312\n"
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss = train(model, data_loaders, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def translate(model, x):\n",
    "#     'translate source sentences into the target language, without looking at the answer'\n",
    "#     with torch.no_grad():\n",
    "#         dB = x.size(0)\n",
    "#         y = torch.tensor([[BOS]*dB]).view(dB, 1).to(device)\n",
    "#         memory = model.encoder(x)\n",
    "#         for i in range(max_seq_len):\n",
    "#             logits, _ = model.decoder(y, memory)\n",
    "#             logits = nn.Softmax(1)(logits)\n",
    "#             last_output = logits.argmax(-1)[:, -1]\n",
    "#             last_output = last_output.view(dB, 1)\n",
    "#             y = torch.cat((y, last_output), 1).to(device)\n",
    "#     return y\n",
    "     \n",
    "# def remove_pad(sent):\n",
    "#     '''truncate the sentence if BOS is in it,\n",
    "#      otherwise simply remove the padding tokens at the end'''\n",
    "#     if sent.count(EOS)>0:\n",
    "#       sent = sent[0:sent.index(EOS)+1]\n",
    "#     while sent and sent[-1] == PAD:\n",
    "#             sent = sent[:-1]\n",
    "#     return sent\n",
    "\n",
    "# def decode_sentence(detokenizer, sentence_ids):\n",
    "#     'convert a tokenized sentence (a list of numbers) to a literal string'\n",
    "#     if not isinstance(sentence_ids, list):\n",
    "#         sentence_ids = sentence_ids.tolist()\n",
    "#     sentence_ids = remove_pad(sentence_ids)\n",
    "#     return detokenizer(sentence_ids).replace(\"\", \"\")\\\n",
    "#            .replace(\"\", \"\").strip().replace(\" .\", \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def translate_this_sentence(text: str):\n",
    "#     'translate the source sentence in string formate into target language'\n",
    "#     input = torch.tensor([[BOS] + tokenizers[SRC](text) + [EOS]]).to(device)\n",
    "#     output = translate(model, input)\n",
    "#     return decode_sentence(detokenizers[TRG], output[0])\n",
    "\n",
    "# translate_this_sentence(\"Eine Gruppe von Menschen steht vor einem Iglu.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
